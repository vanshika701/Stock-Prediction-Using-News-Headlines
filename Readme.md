cat > README.md << 'EOF'
# üì∞ Stock News Scraper & Analysis System

A comprehensive financial news scraping, processing, and analysis system that collects news from multiple sources, detects stock tickers, processes text using NLP, and provides REST APIs for sentiment analysis and UI integration.

---

## üéØ Project Overview

This system automatically:
- Scrapes financial news from 4+ sources (NewsAPI, Alpha Vantage, Finnhub, RSS)
- Detects stock tickers mentioned in articles (97 tickers tracked)
- Cleans and preprocesses text using NLP techniques
- Stores data in PostgreSQL with Redis caching
- Provides REST APIs for team integration
- Runs automatically every 15 minutes

**Built by:** Person 1 (News Retrieval & Preprocessing)  
**Team Integration:** APIs for Person 2 (Sentiment Analysis) & Person 3 (UI/Frontend)

---

## üìä System Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NEWS SCRAPING SYSTEM                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Data Sources:
‚îú‚îÄ‚îÄ NewsAPI          ‚Üí Financial news articles
‚îú‚îÄ‚îÄ Alpha Vantage    ‚Üí Market news with sentiment
‚îú‚îÄ‚îÄ Finnhub          ‚Üí Company-specific news
‚îî‚îÄ‚îÄ RSS Feeds        ‚Üí Yahoo Finance, Reuters, etc.
                ‚Üì
           Unified Scraper
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Text Preprocessing‚îÇ
        ‚îÇ  - Clean HTML     ‚îÇ
        ‚îÇ  - Tokenize       ‚îÇ
        ‚îÇ  - Lemmatize      ‚îÇ
        ‚îÇ  - Extract NER    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Ticker Detection  ‚îÇ
        ‚îÇ  - Find $TICKER   ‚îÇ
        ‚îÇ  - Match companies‚îÇ
        ‚îÇ  - Extract context‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    Storage        ‚îÇ
        ‚îÇ  - PostgreSQL DB  ‚îÇ
        ‚îÇ  - Redis Cache    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    REST APIs      ‚îÇ
        ‚îÇ  - Output API     ‚îÇ
        ‚îÇ  - Input API      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start

### Prerequisites

- Python 3.9+
- PostgreSQL 12+
- Redis 6+
- Virtual environment

### Installation
```bash
# 1. Clone repository
cd ~/Desktop/Projects/Stock-Prediction-using-News-Headlines

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set up environment variables
cp .env.example .env
# Edit .env with your API keys

# 5. Set up database
psql -U postgres -d news_db -f database/schema.sql

# 6. Start Redis
redis-server

# 7. Test the system
python test_integration.py
```

### Running the System

**Option 1: Manual Scrape (One-time)**
```bash
python scrapers/master_scraper.py
```

**Option 2: Automated Scheduler (Continuous)**
```bash
python scheduler/scheduler.py
```

**Option 3: Start APIs for Team**
```bash
# Terminal 1: Output API for Person 2
python api/output_api.py

# Terminal 2: Input API for Person 3
python api/input_api.py
```

---

## üìÅ Project Structure
```
Stock-Prediction-using-News-Headlines/
‚îú‚îÄ‚îÄ api/                           # REST APIs for team integration
‚îÇ   ‚îú‚îÄ‚îÄ output_api.py             # API for Person 2 (Sentiment Analysis)
‚îÇ   ‚îî‚îÄ‚îÄ input_api.py              # API for Person 3 (UI/Frontend)
‚îÇ
‚îú‚îÄ‚îÄ scrapers/                      # News source scrapers
‚îÇ   ‚îú‚îÄ‚îÄ newsapi_scraper.py        # NewsAPI integration
‚îÇ   ‚îú‚îÄ‚îÄ alphavantage_scraper.py   # Alpha Vantage integration
‚îÇ   ‚îú‚îÄ‚îÄ finnhub_scraper.py        # Finnhub integration
‚îÇ   ‚îú‚îÄ‚îÄ rss_scraper.py            # RSS feed parser
‚îÇ   ‚îî‚îÄ‚îÄ master_scraper.py         # Unified scraper
‚îÇ
‚îú‚îÄ‚îÄ preprocessor/                  # Text preprocessing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ text_cleaner.py           # HTML removal, normalization
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py              # Word/sentence tokenization
‚îÇ   ‚îú‚îÄ‚îÄ stop_words.py             # Stop word removal
‚îÇ   ‚îú‚îÄ‚îÄ lemmatizer.py             # Lemmatization
‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor.py      # NER, keywords, dates
‚îÇ   ‚îî‚îÄ‚îÄ duplicate_detector.py     # Duplicate detection
‚îÇ
‚îú‚îÄ‚îÄ utils/                         # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ ticker_database.py        # Stock ticker database
‚îÇ   ‚îú‚îÄ‚îÄ ticker_extractor.py       # Ticker detection algorithm
‚îÇ   ‚îî‚îÄ‚îÄ context_extractor.py      # Context extraction
‚îÇ
‚îú‚îÄ‚îÄ database/                      # Database management
‚îÇ   ‚îú‚îÄ‚îÄ db_manager.py             # PostgreSQL operations
‚îÇ   ‚îú‚îÄ‚îÄ cache_manager.py          # Redis caching
‚îÇ   ‚îî‚îÄ‚îÄ schema.sql                # Database schema
‚îÇ
‚îú‚îÄ‚îÄ scheduler/                     # Automated scheduling
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py              # Main scheduler
‚îÇ   ‚îú‚îÄ‚îÄ error_handler.py          # Error recovery
‚îÇ   ‚îî‚îÄ‚îÄ rate_limiter.py           # API rate limiting
‚îÇ
‚îú‚îÄ‚îÄ config/                        # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ settings.py               # Settings loader
‚îÇ
‚îú‚îÄ‚îÄ logs/                          # Log files
‚îú‚îÄ‚îÄ .env                          # Environment variables (not in git)
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îî‚îÄ‚îÄ README.md                     # This file
```

---

## üîë Environment Variables

Create a `.env` file with:
```bash
# API Keys
NEWSAPI_KEY=your_newsapi_key_here
ALPHAVANTAGE_KEY=your_alphavantage_key_here
FINNHUB_KEY=your_finnhub_key_here

# Database
DATABASE_URL=postgresql://postgres:password@localhost:5432/news_db

# Redis
REDIS_URL=redis://localhost:6379/0

# Settings
FETCH_INTERVAL=900  # 15 minutes in seconds
```

---

## üìä System Statistics

- **Total Articles:** 832 unique articles
- **Data Sources:** 4 (NewsAPI, Alpha Vantage, Finnhub, RSS)
- **Stock Tickers:** 97 tracked
- **Deduplication Rate:** 17% (171 duplicates removed)
- **API Response Time:** 6-10ms
- **Processing Speed:** ~1-2 articles/second

---

## üîó API Endpoints

### Output API (Port 5000) - For Person 2

**Base URL:** `http://localhost:5000`

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/health` | GET | Health check |
| `/api/articles` | GET | Get all articles |
| `/api/articles/ticker/<ticker>` | GET | Get articles by ticker |
| `/api/articles/recent` | GET | Get recent articles (24h) |
| `/api/stream/latest` | GET | Real-time article stream |
| `/api/export/json` | GET | Export articles as JSON |

### Input API (Port 5001) - For Person 3

**Base URL:** `http://localhost:5001`

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/health` | GET | Health check |
| `/api/watchlist` | GET/POST | Manage watchlist |
| `/api/priority` | GET/POST | Set ticker priorities |
| `/api/query` | POST | Submit search query |
| `/api/stats` | GET | Get system statistics |

---

## üß™ Testing
```bash
# Run all tests
python test_integration.py

# Test individual components
python preprocessor/text_cleaner.py
python utils/ticker_extractor.py
python database/db_manager.py

# Test APIs (requires APIs to be running)
curl http://localhost:5000/api/health
curl http://localhost:5001/api/health
```

---

## üìà Data Flow

1. **Scraping** ‚Üí News articles collected from 4 sources
2. **Ticker Detection** ‚Üí Stock symbols identified (97 tickers)
3. **Preprocessing** ‚Üí Text cleaned, tokenized, lemmatized
4. **Storage** ‚Üí Saved to PostgreSQL, cached in Redis
5. **API Access** ‚Üí Available via REST APIs for team

---

## üõ†Ô∏è Troubleshooting

See [TROUBLESHOOTING.md](TROUBLESHOOTING.md) for detailed solutions.

**Common Issues:**

| Issue | Solution |
|-------|----------|
| Database connection failed | Check PostgreSQL is running: `psql -U postgres` |
| Redis connection failed | Start Redis: `redis-server` |
| API key invalid | Verify keys in `.env` file |
| Port already in use | Kill process: `lsof -ti:5000 \| xargs kill -9` |

---

## üìù License

MIT License - See LICENSE file for details

---

## üë• Team

- **Person 1:** News Retrieval & Preprocessing (You!)
- **Person 2:** Sentiment Analysis & Investment Recommendations
- **Person 3:** UI/Frontend Development

---

## üìß Contact

For questions or issues, contact: [Your Email]

Last Updated: November 15, 2024
EOF